{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitness Data Analysis - Modeling and Evaluation\n",
    "\n",
    "This notebook focuses on building and evaluating machine learning models for predicting workout efficiency and calories burned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, r2_score\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_loader import load_fitness_data, split_data\n",
    "from src.preprocessing import identify_column_types, handle_missing_values, create_preprocessing_pipeline, create_workout_efficiency_category, encode_categorical_target\n",
    "from src.feature_engineering import create_all_features\n",
    "from src.models import create_classification_model, create_regression_model, train_model, evaluate_classification_model, evaluate_regression_model, save_model\n",
    "from src.visualization import plot_feature_importance, plot_confusion_matrix\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display all DataFrame columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "data = load_fitness_data()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle missing values\n",
    "data_clean = handle_missing_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply feature engineering\n",
    "data_engineered = create_all_features(data_clean)\n",
    "\n",
    "# Display the engineered dataset\n",
    "print(\"Engineered dataset shape:\", data_engineered.shape)\n",
    "data_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Task: Predicting Workout Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Encode the categorical target for classification\n",
    "data_encoded, label_encoder = encode_categorical_target(data_engineered)\n",
    "\n",
    "# Display encoding mapping\n",
    "print(\"Target encoding mapping:\")\n",
    "mapping = {i: category for i, category in enumerate(label_encoder.classes_)}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify feature types for preprocessing\n",
    "feature_data = data_encoded.drop([c for c in data_encoded.columns if 'Workout_Efficiency' in c], axis=1)\n",
    "numeric_cols, categorical_cols = identify_column_types(feature_data)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = create_preprocessing_pipeline(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data for classification\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(data_encoded, 'Workout_Efficiency_Encoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train Logistic Regression model\n",
    "lr_model = create_classification_model('lr', random_state=42)\n",
    "lr_pipeline = train_model(lr_model, preprocessor, X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set:\")\n",
    "lr_val_results = evaluate_classification_model(lr_pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot confusion matrix for validation set\n",
    "lr_cm_fig = plot_confusion_matrix(y_val, lr_val_results['y_pred'], labels=label_encoder.classes_)\n",
    "plt.title('Logistic Regression - Validation Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train Random Forest model\n",
    "rf_model = create_classification_model('rf', random_state=42)\n",
    "rf_pipeline = train_model(rf_model, preprocessor, X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set:\")\n",
    "rf_val_results = evaluate_classification_model(rf_pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot confusion matrix for validation set\n",
    "rf_cm_fig = plot_confusion_matrix(y_val, rf_val_results['y_pred'], labels=label_encoder.classes_)\n",
    "plt.title('Random Forest - Validation Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance for Random Forest\n",
    "try:\n",
    "    model_instance = rf_pipeline.named_steps['model']\n",
    "    feature_names = numeric_cols + categorical_cols\n",
    "    importance_fig = plot_feature_importance(model_instance, feature_names)\n",
    "    plt.title('Random Forest - Feature Importance')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train XGBoost model\n",
    "xgb_model = create_classification_model('xgb', random_state=42)\n",
    "xgb_pipeline = train_model(xgb_model, preprocessor, X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set:\")\n",
    "xgb_val_results = evaluate_classification_model(xgb_pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot confusion matrix for validation set\n",
    "xgb_cm_fig = plot_confusion_matrix(y_val, xgb_val_results['y_pred'], labels=label_encoder.classes_)\n",
    "plt.title('XGBoost - Validation Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance for XGBoost\n",
    "try:\n",
    "    model_instance = xgb_pipeline.named_steps['model']\n",
    "    feature_names = numeric_cols + categorical_cols\n",
    "    importance_fig = plot_feature_importance(model_instance, feature_names)\n",
    "    plt.title('XGBoost - Feature Importance')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect validation accuracy for comparison\n",
    "model_results = {\n",
    "    'Logistic Regression': lr_val_results['accuracy'],\n",
    "    'Random Forest': rf_val_results['accuracy'],\n",
    "    'XGBoost': xgb_val_results['accuracy']\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_results.keys(), model_results.values())\n",
    "plt.title('Model Accuracy Comparison (Validation Set)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify best model\n",
    "best_model_name = max(model_results, key=model_results.get)\n",
    "print(f\"Best classification model: {best_model_name} with validation accuracy: {model_results[best_model_name]:.4f}\")\n",
    "\n",
    "# Select the best model pipeline\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    best_model_pipeline = lr_pipeline\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model_pipeline = rf_pipeline\n",
    "else:  # XGBoost\n",
    "    best_model_pipeline = xgb_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate best model on test set\n",
    "print(f\"Evaluating {best_model_name} on test set:\")\n",
    "test_results = evaluate_classification_model(best_model_pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot confusion matrix for test set\n",
    "test_cm_fig = plot_confusion_matrix(y_test, test_results['y_pred'], labels=label_encoder.classes_)\n",
    "plt.title(f'{best_model_name} - Test Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the best model\n",
    "best_model_path = save_model(best_model_pipeline, f\"efficiency_classifier_{best_model_name.lower().replace(' ', '_')}\")\n",
    "print(f\"Best model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Task: Predicting Calories Burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Remove efficiency-related columns that were created for classification\n",
    "cols_to_drop = [c for c in data_engineered.columns if 'Workout_Efficiency' in c]\n",
    "data_reg = data_engineered.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify feature types for preprocessing\n",
    "feature_data = data_reg.drop(['Calories_Burned'], axis=1)\n",
    "numeric_cols, categorical_cols = identify_column_types(feature_data)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = create_preprocessing_pipeline(numeric_cols, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data for regression\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(data_reg, 'Calories_Burned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train Linear Regression model\n",
    "lr_reg_model = create_regression_model('lr')\n",
    "lr_reg_pipeline = train_model(lr_reg_model, preprocessor, X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set:\")\n",
    "lr_reg_val_results = evaluate_regression_model(lr_reg_pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, lr_reg_val_results['y_pred'], alpha=0.5)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Calories Burned')\n",
    "plt.ylabel('Predicted Calories Burned')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train Random Forest Regression model\n",
    "rf_reg_model = create_regression_model('rf', random_state=42)\n",
    "rf_reg_pipeline = train_model(rf_reg_model, preprocessor, X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set:\")\n",
    "rf_reg_val_results = evaluate_regression_model(rf_reg_pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, rf_reg_val_results['y_pred'], alpha=0.5)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Calories Burned')\n",
    "plt.ylabel('Predicted Calories Burned')\n",
    "plt.title('Random Forest: Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance for Random Forest Regression\n",
    "try:\n",
    "    model_instance = rf_reg_pipeline.named_steps['model']\n",
    "    feature_names = numeric_cols + categorical_cols\n",
    "    importance_fig = plot_feature_importance(model_instance, feature_names)\n",
    "    plt.title('Random Forest Regression - Feature Importance')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train XGBoost Regression model\n",
    "xgb_reg_model = create_regression_model('xgb', random_state=42)\n",
    "xgb_reg_pipeline = train_model(xgb_reg_model, preprocessor, X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set:\")\n",
    "xgb_reg_val_results = evaluate_regression_model(xgb_reg_pipeline, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, xgb_reg_val_results['y_pred'], alpha=0.5)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Calories Burned')\n",
    "plt.ylabel('Predicted Calories Burned')\n",
    "plt.title('XGBoost: Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance for XGBoost Regression\n",
    "try:\n",
    "    model_instance = xgb_reg_pipeline.named_steps['model']\n",
    "    feature_names = numeric_cols + categorical_cols\n",
    "    importance_fig = plot_feature_importance(model_instance, feature_names)\n",
    "    plt.title('XGBoost Regression - Feature Importance')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect validation R² for comparison\n",
    "reg_model_results = {\n",
    "    'Linear Regression': lr_reg_val_results['r2'],\n",
    "    'Random Forest': rf_reg_val_results['r2'],\n",
    "    'XGBoost': xgb_reg_val_results['r2']\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(reg_model_results.keys(), reg_model_results.values())\n",
    "plt.title('Model R² Comparison (Validation Set)')\n",
    "plt.ylabel('R² Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add R² values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect validation RMSE for comparison\n",
    "reg_model_rmse = {\n",
    "    'Linear Regression': lr_reg_val_results['rmse'],\n",
    "    'Random Forest': rf_reg_val_results['rmse'],\n",
    "    'XGBoost': xgb_reg_val_results['rmse']\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(reg_model_rmse.keys(), reg_model_rmse.values())\n",
    "plt.title('Model RMSE Comparison (Validation Set)')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# Add RMSE values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify best regression model based on R²\n",
    "best_reg_model_name = max(reg_model_results, key=reg_model_results.get)\n",
    "print(f\"Best regression model: {best_reg_model_name} with validation R²: {reg_model_results[best_reg_model_name]:.4f}\")\n",
    "\n",
    "# Select the best model pipeline\n",
    "if best_reg_model_name == 'Linear Regression':\n",
    "    best_reg_pipeline = lr_reg_pipeline\n",
    "elif best_reg_model_name == 'Random Forest':\n",
    "    best_reg_pipeline = rf_reg_pipeline\n",
    "else:  # XGBoost\n",
    "    best_reg_pipeline = xgb_reg_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate best model on test set\n",
    "print(f\"Evaluating {best_reg_model_name} on test set:\")\n",
    "reg_test_results = evaluate_regression_model(best_reg_pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot actual vs predicted values for test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, reg_test_results['y_pred'], alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Calories Burned')\n",
    "plt.ylabel('Predicted Calories Burned')\n",
    "plt.title(f'{best_reg_model_name}: Actual vs Predicted (Test Set)')\n",
    "\n",
    "# Add R² and RMSE to plot\n",
    "r2 = reg_test_results['r2']\n",
    "rmse = reg_test_results['rmse']\n",
    "plt.annotate(f'R² = {r2:.4f}\\nRMSE = {rmse:.2f}', \n",
    "             xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the best regression model\n",
    "best_reg_model_path = save_model(best_reg_pipeline, f\"calories_burned_regressor_{best_reg_model_name.lower().replace(' ', '_')}\")\n",
    "print(f\"Best regression model saved to: {best_reg_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook has demonstrated the development and evaluation of machine learning models for the fitness data analysis project. We've successfully built models for two prediction tasks:\n",
    "\n",
    "### 1. Classification Task: Predicting Workout Efficiency\n",
    "- We've trained and compared Logistic Regression, Random Forest, and XGBoost models\n",
    "- The best model achieved good accuracy in classifying workout efficiency into Low, Medium, and High categories\n",
    "- Key predictive features were identified through feature importance analysis\n",
    "\n",
    "### 2. Regression Task: Predicting Calories Burned\n",
    "- We've trained and compared Linear Regression, Random Forest, and XGBoost regression models\n",
    "- The best model achieved a strong R² score in predicting calories burned\n",
    "- Feature importance analysis revealed the most influential factors in calorie expenditure\n",
    "\n",
    "These models provide valuable insights into fitness performance prediction and can help individuals understand which factors most significantly impact their workout results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}